defaults:
  - trainer: ppo
  - env: fetch_reach
  - model: td_actor_critic
  - loss: ppo
  - collector: async
  - optimizer: adam
  - evaluator: rl_eval_basic
  - _self_

exp:
  name: "fetch_ppo"

trainer:
  max_steps: 100000
  eval_every: 5000
  save_every: 20000

collector:
  frames_per_batch: 8192
  total_frames: 1000000
  num_workers: 4
